# export CUDA_VISIBLE_DEVICES=0
# nohup python capacity_sample_PiERN.py > PiERN_1.0B_capacity.log 2>&1 &

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
import time
import pynvml
import jsonlines

pynvml.nvmlInit()
gpu_index = 0
handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)  # 0å·GPU

# -----------------------------
# é…ç½®
# -----------------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# please download the model at https://www.modelscope.cn/models/Qwen/Qwen2.5-0.5B-Instruct/files
BASE_MODEL_PATH = "/data/models/Qwen/Qwen2.5-0.5B-Instruct"

# please download the model at https://huggingface.co/HengBooo233/PiERN/tree/main
ROUTER_WEIGHTS = "../model/capacity_token_router.pt"                         
LM13D_WEIGHTS = "../model/capacity_test2computation_module.pt"   

   
DEEPONET_WEIGHTS = "../model/capacity_expert_model.pt"


# -----------------------------
# æ¨¡å‹å®šä¹‰
# -----------------------------
class LMClassifier1D(nn.Module):
    def __init__(self, vocab_size, embed_dim=1536, hidden_dim=128, output_dim=1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.fc = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, input_ids, attention_mask):
        embedded = self.embedding(input_ids)  # [B, T, E]
        masked = embedded * attention_mask.unsqueeze(-1)
        pooled = masked.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)  # å¹³å‡æ± åŒ–
        logits = self.fc(pooled)  # [B, 1]
        return logits
    

class DeepONet(nn.Module):
    """DeepONet: SoH å›å½’"""
    def __init__(self, n, dim):
        super(DeepONet, self).__init__()
        self.n_branch_net = nn.Sequential(
            nn.Linear(n, 2048), nn.ReLU(), nn.Dropout(p=0.02),
            nn.Linear(2048, 1024), nn.ReLU(),
            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(p=0.02),
            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(p=0.02),
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(p=0.02),
            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(p=0.02),
            nn.Linear(64, 10)
        )
        self.chunk_net = nn.Sequential(
            nn.Linear(dim, 784), nn.ReLU(), nn.Dropout(p=0.02),
            nn.Linear(784, 512), nn.ReLU(), nn.Dropout(p=0.02),
            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(p=0.02),
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(p=0.02),
            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(p=0.02),
            nn.Linear(64, 10)
        )
        self.output_net = nn.Sequential(nn.Linear(10, 1))

    def forward(self, x):
        n_branch_inputs, chunk_inputs = x[0], x[1]
        n_branch_outputs = self.n_branch_net(n_branch_inputs)
        chunk_outputs = self.chunk_net(chunk_inputs)
        outputs = torch.sum(n_branch_outputs * chunk_outputs, dim=1, keepdim=True)
        return outputs


class LMRegression13D(nn.Module):
    def __init__(self, base_model, output_dim=13):
        super().__init__()
        self.base_model = base_model
        self.hidden_size = base_model.model.embed_tokens.embedding_dim
        self.head = nn.Sequential(
            nn.Linear(self.hidden_size, 128),
            nn.ReLU(),
            nn.Linear(128, output_dim)
        )

    def forward(self, input_ids, attention_mask):
        outputs = self.base_model.model(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden = outputs.last_hidden_state  # [B, T, H]
        masked = last_hidden * attention_mask.unsqueeze(-1)
        pooled = masked.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)
        return self.head(pooled)  # [B, 14]
    

# -----------------------------
# åˆå§‹åŒ–æ¨¡å‹ï¼ˆä¸€æ¬¡æ€§åŠ è½½ï¼‰
# -----------------------------
print("Loading models...")

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, use_fast=False, trust_remote_code=True)

vocab_size = len(tokenizer)
router_model = LMClassifier1D(vocab_size).to(DEVICE)
router_model.load_state_dict(torch.load(ROUTER_WEIGHTS, map_location=DEVICE))
router_model.eval()

base_model_lm = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True).to(DEVICE)
lm_model = LMRegression13D(base_model_lm).to(DEVICE)
lm_model.load_state_dict(torch.load(LM13D_WEIGHTS, map_location=DEVICE))
lm_model.eval()

deeponet = torch.load(DEEPONET_WEIGHTS, map_location=DEVICE).to(DEVICE)
deeponet.eval()

llm_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True).to(DEVICE)
llm_model.eval()

print("All models loaded.")


# -----------------------------
# å·¥å…·å‡½æ•°
# -----------------------------
def inference_router_from_ids(input_ids, attention_mask):
    """é«˜æ•ˆ Router æ¨ç†ï¼šç›´æ¥è¾“å…¥ token_idsï¼Œä¸éœ€è¦ decode/encode"""
    with torch.no_grad():
        logits = router_model(input_ids, attention_mask)
        probs = torch.sigmoid(logits).squeeze(-1)  # [B]
        preds = (probs > 0.5).long()
    return preds.item(), probs.item()



def generate_response_deeponet_from_ids(input_ids, attention_mask) -> torch.Tensor:
    """è¿”å›å¼ é‡ [B]ï¼ˆæ¯æ¡æ ·æœ¬ä¸€ä¸ª SoH æ•°å€¼ï¼‰"""
    with torch.no_grad():
        preds_14d = lm_model(input_ids, attention_mask)       # [B, 14]
        print("!!!!!!", preds_14d)
        branch_in, chunk_in = preds_14d[:, :11], preds_14d[:, 11:]  # [B,11], [B,3]
        chunk_in_swapped = chunk_in[:, [1, 0]]   # æ‰‹åŠ¨äº¤æ¢åˆ—
        outputs = deeponet((branch_in, chunk_in_swapped))             # [B,1]
        soh_vals = outputs.squeeze(-1)                        # [B]
    return soh_vals


def interpret_soh_value(soh_value: float) -> str:
    if soh_value > 0.8:
        return "ç”µæ± å¥åº·åº¦è‰¯å¥½ã€‚"
    elif soh_value < 0.6:
        return "ç”µæ± ä¸¥é‡è¡°å‡ã€‚"
    else:
        return "ç”µæ± çŠ¶æ€ä¸€èˆ¬ã€‚"


def generate_response_with_router(messages, tokenizer, llm_model, device, max_new_tokens=50):
    """é€æ­¥è§£ç  + é«˜æ•ˆ Router åˆ¤åˆ«"""
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt", padding=True, truncation=True).to(device)
    generated_ids = inputs["input_ids"]
    
    prompt_len = inputs["input_ids"].size(1)   # åˆå§‹ prompt é•¿åº¦

    for _ in range(max_new_tokens):
        with torch.no_grad():
            # 1. æ­£å¸¸ç”Ÿæˆä¸‹ä¸€ä¸ª token
            outputs = llm_model(input_ids=generated_ids)
            next_token_logits = outputs.logits[:, -1, :]
            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)

            # 2. EOS åœæ­¢
            if next_token.item() == tokenizer.eos_token_id:
                break

            # 3. æ‹¼æ¥
            generated_ids = torch.cat([generated_ids, next_token], dim=-1)

            # 4. Router åˆ¤åˆ«ï¼ˆç›´æ¥åƒ token_idsï¼Œä¸ decodeï¼‰
            new_tokens = generated_ids[:, prompt_len:]  # åªä¿ç•™æ–°ç”Ÿæˆéƒ¨åˆ†
            attention_mask = torch.ones_like(new_tokens).to(device)
            pred, prob = inference_router_from_ids(new_tokens, attention_mask)
            
            # print("***************", pred)

            if pred == 1:
                # ğŸš¨ Router è§¦å‘ â†’ åˆ‡åˆ° DeepONet
                attention_mask = torch.ones_like(generated_ids).to(device)
                soh_vals = generate_response_deeponet_from_ids(generated_ids, attention_mask)  # [B]
                soh_value = float(soh_vals[0].item())
                soh_str = f"{soh_value:.6f}ã€‚ç»“è®ºï¼š{interpret_soh_value(soh_value)}"
                # soh_str = f"{soh_value:.6f}ã€‚ç»“è®ºï¼š"
                soh_ids = tokenizer.encode(soh_str, add_special_tokens=False, return_tensors="pt").to(device)

                generated_ids = torch.cat([generated_ids, soh_ids], dim=-1)
                # break  # å·²å®Œæˆï¼Œé€€å‡ºå¾ªç¯

    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)



if __name__ == "__main__":
    
    results_path = "PiERN_1.0B_capacity.txt"

    with open(results_path, "w", encoding="utf-8") as fout, jsonlines.open("../data/capacity_sample_PiERN.jsonl", "r") as reader:
        for idx, record in enumerate(reader):
            if idx >= 2:   # ğŸ‘ˆ é™åˆ¶æ¡æ•°
                break

            if not isinstance(record, dict) or "prompt" not in record:
                continue
            message = str(record["prompt"]).strip()
            if not message:
                continue

            # æ„å»º messages
            messages_language = [
                {"role": "system", "content": '''
                ä½ æ˜¯ä¸€åç»“åˆè¯­è¨€ç†è§£èƒ½åŠ›ä¸ç‰©ç†å»ºæ¨¡èƒ½åŠ›çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·è¾“å…¥çµæ´»åˆ‡æ¢ä»»åŠ¡æ¨¡å¼ã€‚  
                å½“ç”¨æˆ·æå‡ºçš„æ˜¯é”‚ç”µæ± å¥åº·åº¦ï¼ˆState of Health, SoHï¼‰é¢„æµ‹ç±»çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼šâ€œè¯·é¢„æµ‹SoH1.0çš„é”‚ç”µæ± åœ¨[...]çš„2å°æ—¶ç”µæµä½œç”¨ä¸‹çš„å¥åº·åº¦å˜åŒ–â€ï¼Œä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š  
                1. å›ç­”å¼€å§‹æ—¶åªèƒ½è¾“å‡ºè¯­è¨€ + <æ•°å€¼è®¡ç®—ç»“æœ>ã€‚ç»“è®º:
                2. åœ¨SoHé¢„æµ‹ç±»é—®é¢˜ä¸­ï¼Œå¦‚æœè·å¾—å…·ä½“æ•°å€¼ï¼Œåˆ™æ ¹æ®ä»¥ä¸‹æ¡ä»¶è¡¥å……è¯´æ˜ç”µæ± å¥åº·åº¦ç»“è®ºï¼š  
                -   å¦‚æœ `è®¡ç®—ç»“æœ > 0.8`ï¼Œåˆ™è¾“å‡ºï¼š`ã€‚ç»“è®º:ç”µæ± çŠ¶æ€è‰¯å¥½ã€‚`
                -   å¦‚æœ `è®¡ç®—ç»“æœ < 0.6`ï¼Œåˆ™è¾“å‡ºï¼š`ã€‚ç»“è®º:ç”µæ± ä¸¥é‡è¡°å‡ã€‚`
                -   å¦‚æœ `0.6 <= è®¡ç®—ç»“æœ <= 0.8`ï¼Œåˆ™è¾“å‡ºï¼š`ã€‚ç»“è®º:ç”µæ± çŠ¶æ€ä¸€èˆ¬ã€‚`   
                3. å›ç­”ç¤ºä¾‹ï¼šâ€œç»è¿‡æ¨ç†ï¼Œé¢„è®¡è¯¥æ—¶åˆ»ç”µæ± çš„å¥åº·åº¦ä¸º<æ•°å€¼è®¡ç®—ç»“æœ>ã€‚ç»“è®º:â€  
                4. å¦‚æœç”¨æˆ·æå‡ºçš„æ˜¯æ™®é€šè¯­è¨€å¯¹è¯ï¼ˆå¦‚â€œä½ æ˜¯è°â€æˆ–â€œä½ å¥½â€ï¼‰ï¼Œåˆ™æŒ‰æ™®é€šå¯¹è¯æ­£å¸¸å›ç­”
                '''},
                {"role": "user", "content": message}
            ]

            # === ç»Ÿè®¡å¼€å§‹ ===
            start_time = time.time()
            start_energy = pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)

            result = generate_response_with_router(messages_language, tokenizer, llm_model, DEVICE, max_new_tokens=100)

            # === ç»Ÿè®¡ç»“æŸ ===
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            end_time = time.time()
            latency = end_time - start_time

            end_energy = pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)
            energy_J = (end_energy - start_energy) / 1000.0
            
            tokens_response = len(tokenizer.encode(result, add_special_tokens=False))

            # å†™ç»“æœ
            fout.write(f"Index {idx}\n")
            fout.write(f"Latency: {latency:.6f} s\n")
            fout.write(f"Tokens: response={tokens_response}, total={tokens_response}\n")
            fout.write(f"GPU{gpu_index} Energy: {energy_J:.6f} J\n")
            fout.write(f"Response: {result}\n")
            fout.write("-" * 40 + "\n")

    pynvml.nvmlShutdown()
    print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° {results_path}")